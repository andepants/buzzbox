# Story 9.3: RAG-Enhanced AI Drafts with Memory Search

**Epic:** Epic 9 - Supermemory RAG Integration
**Story ID:** 9.3
**Priority:** P1 (High)
**Estimated Time:** 3-4 hours
**Dependencies:** Story 9.1 (Supermemory Service), Story 9.2 (Memory Storage)
**Status:** Draft

---

## 📖 User Story

**As** Andrew (the creator),
**I want** AI-generated reply drafts to include context from similar past conversations,
**So that** the AI suggestions are more accurate, consistent, and reflective of my actual communication style.

---

## 🎯 Acceptance Criteria

### Functional Requirements

1. **Memory Search Before AI Generation**
   - [ ] Call `SupermemoryService.shared.searchMemories()` before generating AI draft
   - [ ] Use fan's message text as the search query
   - [ ] Request top 3 most relevant memories (limit: 3)
   - [ ] Apply 2-second timeout (graceful fallback on timeout)
   - [ ] Continue with AI generation even if search fails

2. **Prompt Enhancement with Context**
   - [ ] If memories found: Include Q&A pairs in system prompt
   - [ ] If no memories: Use existing prompt (no changes)
   - [ ] Format memories as "Past Conversations" section
   - [ ] Preserve existing conversation history context
   - [ ] Maintain existing temperature and token limits

3. **Context Formatting**
   - [ ] Add section header: "Here are similar past conversations:"
   - [ ] Separate each Q&A pair with blank lines
   - [ ] Preserve original Q&A format from memories
   - [ ] Limit total context to 500 tokens (truncate if needed)

4. **Graceful Degradation**
   - [ ] If Supermemory disabled: Generate draft normally
   - [ ] If search times out: Generate draft without memories
   - [ ] If search fails: Log error, generate draft without memories
   - [ ] Never show Supermemory errors to user

### Integration Requirements

5. **AIService Modification**
   - [ ] Update `generateSmartReply()` in `Core/Services/AIService.swift`
   - [ ] Add memory search step before OpenAI call
   - [ ] Enhance system prompt with memory context
   - [ ] Preserve existing reply generation logic
   - [ ] Maintain backward compatibility

6. **Performance Requirements**
   - [ ] Total AI draft generation time: <4 seconds (including search)
   - [ ] Supermemory search timeout: 2 seconds max
   - [ ] No blocking of UI thread
   - [ ] Handle concurrent requests gracefully

### Code Quality Requirements

7. **Implementation Quality**
   - [ ] Add `// MARK: - RAG Context Enhancement` section
   - [ ] Extract prompt building into helper method
   - [ ] Add doc comments explaining RAG pattern
   - [ ] Keep code readable and maintainable

8. **Testing**
   - [ ] Test with 0 memories: Works like before
   - [ ] Test with 3 memories: Context included
   - [ ] Test with Supermemory disabled: No crash
   - [ ] Test with network failure: Graceful fallback

---

## 🔧 Technical Implementation Notes

### Integration Point

**File:** `Core/Services/AIService.swift`

**Method:** `generateSmartReply(for message: Message, in conversation: Conversation) async throws -> [String]`

### Code Implementation

```swift
// MARK: - RAG Context Enhancement

/// Searches Supermemory for relevant past conversations
/// Returns formatted context string for prompt enhancement
private func fetchRAGContext(for message: Message) async -> String? {
    // Only if Supermemory is enabled
    guard SupermemoryService.shared.isEnabled else {
        return nil
    }

    do {
        // Search with 2-second timeout
        let memories = try await withTimeout(seconds: 2) {
            try await SupermemoryService.shared.searchMemories(
                query: message.text,
                limit: 3
            )
        }

        guard !memories.isEmpty else {
            return nil
        }

        // Format memories as context
        let contextSection = """

        Here are similar past conversations for context:

        \(memories.map { $0.content }.joined(separator: "\n\n"))
        """

        return contextSection

    } catch {
        print("⚠️ RAG context fetch failed: \(error)")
        return nil // Graceful degradation
    }
}

/// Builds enhanced system prompt with optional RAG context
private func buildSystemPrompt(ragContext: String?) -> String {
    var prompt = """
    You are Andrew's AI assistant. Generate a reply in his voice.

    Andrew's Style: Friendly, encouraging, tech-focused, uses emojis occasionally.
    """

    if let ragContext = ragContext {
        prompt += ragContext
    }

    return prompt
}

/// Helper for timeout implementation
private func withTimeout<T>(seconds: TimeInterval, operation: @escaping () async throws -> T) async throws -> T {
    try await withThrowingTaskGroup(of: T.self) { group in
        group.addTask {
            try await operation()
        }

        group.addTask {
            try await Task.sleep(nanoseconds: UInt64(seconds * 1_000_000_000))
            throw SupermemoryError.timeout
        }

        let result = try await group.next()!
        group.cancelAll()
        return result
    }
}
```

### Updated generateSmartReply() Method

```swift
func generateSmartReply(for message: Message, in conversation: Conversation) async throws -> [String] {
    // Step 1: Fetch RAG context from Supermemory (with timeout)
    let ragContext = await fetchRAGContext(for: message)

    if ragContext != nil {
        print("✅ Using RAG context for AI draft")
    } else {
        print("ℹ️ Generating AI draft without RAG context")
    }

    // Step 2: Build enhanced system prompt
    let systemPrompt = buildSystemPrompt(ragContext: ragContext)

    // Step 3: Build conversation history (existing logic)
    let conversationHistory = buildConversationHistory(conversation: conversation)

    // Step 4: Call OpenAI with enhanced prompt (existing logic)
    let response = try await openAI.chat.completions.create(
        model: "gpt-4o-mini",
        messages: [
            .system(content: systemPrompt),
            .user(content: conversationHistory + "\n\n" + message.text)
        ],
        temperature: 0.7,
        maxTokens: 200,
        n: 3 // Generate 3 reply options
    )

    // Step 5: Extract and return replies (existing logic)
    return response.choices.map { $0.message.content ?? "" }
}
```

---

## 📊 Example: Before vs After

### Before (Without RAG)

**Fan Message:** "when do you stream?"

**AI Draft:**
> "I stream regularly! Check my schedule for upcoming sessions."

**Problem:** Generic, no specific details

---

### After (With RAG)

**Fan Message:** "when do you stream?"

**Supermemory Search Results:**
```
Q: When is your next livestream?
A: Next livestream is Saturday at 3pm EST! I'll be coding a SwiftUI app 🎉

Q: Do you have a streaming schedule?
A: I stream every Saturday at 3pm EST. Sometimes I add extra sessions on Wednesdays!
```

**Enhanced System Prompt:**
```
You are Andrew's AI assistant. Generate a reply in his voice.

Andrew's Style: Friendly, encouraging, tech-focused, uses emojis occasionally.

Here are similar past conversations for context:

Q: When is your next livestream?
A: Next livestream is Saturday at 3pm EST! I'll be coding a SwiftUI app 🎉

Q: Do you have a streaming schedule?
A: I stream every Saturday at 3pm EST. Sometimes I add extra sessions on Wednesdays!
```

**AI Draft (RAG-Enhanced):**
> "Next stream is Saturday at 3pm EST! I'll be building something cool with SwiftUI 🎉"

**Result:** ✅ Specific time, ✅ Andrew's style, ✅ Accurate information

---

## 🔗 Related Files

- **Modify:** `Core/Services/AIService.swift` - Add RAG context fetching
- **Reference:** `Core/Services/SupermemoryService.swift` - Call searchMemories()
- **Reference:** `Core/Models/Memory.swift` - Memory data structure
- **Test with:** `Core/ViewModels/MessageThreadViewModel.swift` - Triggers AI drafts

---

## 📋 Definition of Done

- [ ] `AIService.generateSmartReply()` includes memory search
- [ ] Helper method `fetchRAGContext()` implemented with timeout
- [ ] Helper method `buildSystemPrompt()` implemented
- [ ] Helper method `withTimeout()` implemented
- [ ] All acceptance criteria met and tested
- [ ] Code compiles without warnings
- [ ] Graceful degradation if Supermemory unavailable
- [ ] Performance within acceptable limits (<4s total)
- [ ] Manual testing shows improved AI responses
- [ ] No regressions in existing AI draft functionality

---

## ⚠️ Important Notes

1. **Backward Compatibility:** Must work exactly as before if Supermemory disabled
2. **Performance:** 2-second timeout critical to prevent UI hangs
3. **No User Errors:** Never show Supermemory failures to user
4. **Context Limits:** Don't exceed OpenAI token limits with too much context
5. **Quality Over Quantity:** 3 high-quality memories better than 10 low-quality ones

---

## 🔍 Testing Guidance

### Manual Testing Steps

**Test 1: RAG-Enhanced Draft (Happy Path)**
1. Setup: Complete Story 9.1 and 9.2
2. As Andrew, manually answer several similar questions (e.g., streaming schedule)
3. Wait 5 minutes for Supermemory indexing
4. As fan, send similar question: "when do you stream?"
5. Click "Smart Reply" button
6. ✅ Verify console: "✅ Using RAG context for AI draft"
7. ✅ Verify draft includes specific details from past answers

**Test 2: No Memories Found**
1. As fan, send unique question: "what's your favorite color?"
2. Click "Smart Reply"
3. ✅ Verify console: "ℹ️ Generating AI draft without RAG context"
4. ✅ Draft still generates (uses existing logic)

**Test 3: Supermemory Disabled**
1. Remove API key from Settings
2. Send message as fan
3. Click "Smart Reply"
4. ✅ Draft generates without RAG (no crash)

**Test 4: Network Timeout**
1. Add network delay (Charles Proxy or similar)
2. Click "Smart Reply"
3. ✅ After 2 seconds, draft generates without memories
4. ✅ Console: "⚠️ RAG context fetch failed: timeout"

**Test 5: Compare Quality**
1. Generate 10 drafts WITHOUT RAG (disable Supermemory)
2. Generate 10 drafts WITH RAG (enable Supermemory)
3. ✅ RAG drafts should be more specific and consistent

---

## 🎓 Technical Deep Dive

### RAG Pattern Explained

**RAG = Retrieval-Augmented Generation**

1. **Retrieval:** Search memory database for relevant past conversations
2. **Augmentation:** Add retrieved context to AI prompt
3. **Generation:** AI generates response using both current message and past context

**Why RAG Improves AI Quality:**
- Reduces hallucination (AI makes up facts)
- Ensures consistency across similar questions
- Learns from real examples (Andrew's actual style)
- No need to manually update FAQs

**Trade-offs:**
- ✅ Better accuracy and consistency
- ✅ Automatic learning over time
- ⚠️ Adds 500ms-2s latency
- ⚠️ Requires internet for search
- ⚠️ External API dependency

---

**Created by:** Sarah (PO Agent)
**Date:** 2025-10-26
**Depends on:** Stories 9.1 and 9.2 must be completed first
