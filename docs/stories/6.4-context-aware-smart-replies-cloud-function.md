# Story 6.4: Context-Aware Smart Replies Cloud Function (Feature 2 + Advanced)

## Status
Draft

## Story
**As** Andrew (the creator),
**I want** AI to draft replies in my voice with conversation context,
**so that** I can respond faster and more authentically to fan messages.

## Acceptance Criteria
1. Cloud Function fetches conversation context (last 20 messages)
2. Creator profile with style examples stored in Firestore
3. GPT-4o-mini generates 3 distinct reply options
4. iOS shows smart reply picker with 3 options
5. Selecting a draft populates message composer (editable)
6. Response time <3s (GPT-4o-mini is 3x faster than GPT-4!)
7. Drafts sound authentic to Andrew's voice
8. Drafts use conversation context (not generic)
9. Loading state shows while generating
10. Error handling with user-friendly messages

### Advanced AI Capability Requirements
- âœ… Learns user style accurately (from examples + profile)
- âœ… Generates authentic-sounding replies (GPT-4o-mini with style training)
- âœ… Provides 3+ relevant options (short/medium/detailed)
- âœ… Response times EXCEED targets (<3s vs <8s requirement!)
- âœ… Context-aware (uses recent messages)

## Tasks / Subtasks

- [ ] Create and validate creator profile in Firestore (AC: 2)
  - [ ] Manually create document in Firebase Console
  - [ ] Path: `/creator_profiles/andrew` (document ID matches creator identifier)
  - [ ] Required fields:
    - `personality` (string, min 10 chars)
    - `tone` (string, min 10 chars)
    - `examples` (array, minimum 5 items)
    - `avoid` (array, minimum 3 items)
    - `signature` (string)
    - `created_at` (timestamp for tracking)
  - [ ] Use template from Epic 6 PRD
  - [ ] Validate examples array has at least 5 authentic responses
  - [ ] Verify tone and personality are descriptive

- [ ] Create smart replies Cloud Function file
  - [ ] Create `functions/src/smart-replies.ts`
  - [ ] Import: `firebase-functions/v2/https`, `firebase-admin`, `openai`
  - [ ] Initialize OpenAI client
  - [ ] Define interfaces: Message, CreatorProfile, SmartReplyResponse, SmartReplyRequest

- [ ] Implement generateSmartReplies callable function (AC: 1)
  - [ ] Create `generateSmartReplies` using `onCall`
  - [ ] Accept `conversationId` and `messageText` from request.data
  - [ ] Validate both parameters are provided
  - [ ] Throw HttpsError if invalid input

- [ ] Fetch conversation context from RTDB (AC: 1, 8)
  - [ ] Query RTDB: `admin.database().ref(\`/messages/\${conversationId}\`)`
  - [ ] Use `orderByChild('timestamp').limitToLast(20)`
  - [ ] Filter out messages without timestamps (skip malformed data)
  - [ ] Map snapshot to Message array
  - [ ] Include both sender and receiver messages for full context
  - [ ] Sort by timestamp ascending (oldest first) for chronological order
  - [ ] Format as: `[SENDER_NAME]: {text}` for each message

- [ ] Fetch and cache creator profile from Firestore (AC: 2)
  - [ ] Query Firestore: `admin.firestore().collection('creator_profiles').doc('andrew').get()`
  - [ ] Throw HttpsError('not-found', 'Creator profile not configured') if doesn't exist
  - [ ] Log profile fetch success for monitoring
  - [ ] Extract personality, tone, examples, avoid, signature
  - [ ] Cache profile in memory for 5 minutes to reduce Firestore reads
  - [ ] Use global variable or module-level cache

- [ ] Build context-aware system prompt (AC: 7, 8)
  - [ ] Format conversation context: `{senderName}: {text}`
  - [ ] Build system prompt with: creator personality, tone, example responses, things to avoid
  - [ ] Include recent conversation context in system prompt
  - [ ] System: "You are Andrew, a {personality}. Your tone: {tone}. Example responses: {examples}. Avoid: {avoid}. Recent conversation: {context}"

- [ ] Build user prompt for 3 reply options (AC: 3)
  - [ ] User prompt: "The fan just sent: {messageText}"
  - [ ] Request 3 reply options: short (1 sentence), medium (2-3 sentences), detailed (4-5 sentences)
  - [ ] Instruct to make each option sound authentic to Andrew's voice
  - [ ] Instruct to use conversation context for relevance
  - [ ] Request JSON response format: `{ "short": "...", "medium": "...", "detailed": "..." }`
  - [ ] Add: "IMPORTANT: Respond with ONLY valid JSON, no additional text"
  - [ ] Add: "Use double quotes for all strings. Ensure proper JSON formatting."

- [ ] Call OpenAI GPT-4o-mini (AC: 6)
  - [ ] Model: `gpt-4o-mini`
  - [ ] Messages: system prompt + user prompt
  - [ ] Temperature: 0.7 (creative but controlled)
  - [ ] Response format: `{ type: 'json_object' }`
  - [ ] Parse JSON response

- [ ] Validate and return response with fallback (AC: 3)
  - [ ] Parse JSON from GPT response
  - [ ] Verify all required fields exist: `short`, `medium`, `detailed`
  - [ ] Verify each field is non-empty string
  - [ ] If any field missing, throw error with clear message
  - [ ] If JSON parsing fails, retry once with lower temperature (0.5)
  - [ ] Return `{ drafts: { short, medium, detailed } }`
  - [ ] Log validation failures for monitoring

- [ ] Add error handling (AC: 10)
  - [ ] Wrap in try/catch block
  - [ ] Log errors with `logger.error()` including context
  - [ ] Throw HttpsError('internal', 'Smart reply generation failed')
  - [ ] Client will show user-friendly error message

- [ ] Export function and deploy
  - [ ] Export `generateSmartReplies` in `functions/src/index.ts`
  - [ ] Deploy: `firebase deploy --only functions`
  - [ ] Verify deployment succeeds

- [ ] Test with real conversation context (AC: 6, 7, 8)
  - [ ] Create test conversation with multiple messages
  - [ ] Call function from iOS app (or Firebase Console test)
  - [ ] Verify 3 distinct reply options are returned
  - [ ] Verify responses sound like Andrew's voice
  - [ ] Verify responses use conversation context
  - [ ] Measure response time (should be <3 seconds)
  - [ ] Check function logs for performance

## Dev Notes

### Architecture Context

**AI Feature Flow** [Source: architecture/ai-integration-architecture.md#6.1]
- On-Demand (Callable) smart reply generation
- User taps "Draft Reply" button in iOS app
- iOS app calls Cloud Function with conversationId and messageText
- Cloud Function fetches context from RTDB (last 20 messages)
- Cloud Function fetches creator's writing style from Firestore
- Builds prompt with context + style, calls OpenAI GPT-4o-mini
- Returns 3 generated drafts to iOS app
- iOS displays drafts in editable card (user can edit/send/dismiss)

**Cloud Functions Specifications** [Source: architecture/ai-integration-architecture.md#6.2]
- Function 2: `generateSmartReplyCallable` (this story)
  - Fetch context from RTDB (last 20 messages)
  - Fetch creator's voice patterns from Firestore
  - Generate personalized reply via GPT-4o-mini
  - Target execution: ~3 seconds

**Advanced AI Capability** [Source: docs/prd/epic-6-ai-powered-creator-inbox.md]
- Context-Aware Smart Replies requirement (10 points)
- Must learn user style accurately
- Must generate authentic-sounding replies
- Must provide 3+ relevant options
- Must have response times <8s (we're targeting <3s!)
- This story satisfies Feature 2 + Advanced AI Capability

### Creator Profile Structure

**Firestore Document: `/creator_profiles/andrew`**

```json
{
  "personality": "Friendly tech content creator. Casual but professional. Authentic and enthusiastic about helping fans.",
  "tone": "warm, encouraging, uses emojis occasionally",
  "examples": [
    "Hey! Thanks so much for reaching out! ðŸ™Œ",
    "That's awesome! I'd love to hear more about your project.",
    "Appreciate the kind words, it really means a lot!",
    "Great question! Here's how I approach that...",
    "Let me know if you need anything else, always happy to help!"
  ],
  "avoid": [
    "Overly formal language",
    "Corporate speak",
    "Robotic responses",
    "Generic templates"
  ],
  "signature": "- Andrew"
}
```

### Database Strategy

**RTDB for Messages** [Source: architecture/technology-stack.md#2.2]
- Real-time Database: Chat messages, typing indicators, presence
- Path: `/messages/{conversationId}/{messageId}`
- Query last 20 messages: `orderByChild('timestamp').limitToLast(20)`

**Firestore for Profiles** [Source: architecture/technology-stack.md#2.2]
- Firestore: User profiles, static data
- Path: `/creator_profiles/{profileId}`
- Creator profile is static (doesn't change often)

### File Locations

**New File to Create:**
- `functions/src/smart-replies.ts` - Smart Replies Cloud Function implementation

**Files to Modify:**
- `functions/src/index.ts` - Export `generateSmartReplies` function

**Firestore Document:**
- `/creator_profiles/andrew` - Created manually in Firebase Console

### Implementation Details

**Function Signature:**
```typescript
export const generateSmartReplies = onCall<SmartReplyRequest>({
  region: 'us-central1',
}, async (request) => {
  const { conversationId, messageText } = request.data;
  // Implementation here
});
```

**Conversation Context Building:**
```typescript
const conversationContext = messages
  .map((m) => `${m.senderName}: ${m.text}`)
  .join('\n');
```

**System Prompt Template:**
```typescript
const systemPrompt = `You are Andrew, a ${profile.personality}

Your tone: ${profile.tone}

Example responses you've written:
${profile.examples.join('\n')}

Avoid: ${profile.avoid.join(', ')}

Recent conversation context:
${conversationContext}`;
```

**User Prompt Template:**
```typescript
const userPrompt = `The fan just sent: "${messageText}"

Generate 3 reply options:
1. Short (1 sentence, quick acknowledgment)
2. Medium (2-3 sentences, friendly and helpful)
3. Detailed (4-5 sentences, comprehensive response)

Make each option sound authentic to Andrew's voice. Use conversation context to make replies relevant.

Respond with ONLY valid JSON in this exact format:
{
  "short": "your short reply here",
  "medium": "your medium reply here",
  "detailed": "your detailed reply here"
}`;
```

**OpenAI Call:**
```typescript
const completion = await openai.chat.completions.create({
  model: 'gpt-4o-mini',
  messages: [
    { role: 'system', content: systemPrompt },
    { role: 'user', content: userPrompt },
  ],
  temperature: 0.7,
  response_format: { type: 'json_object' },
});
```

**Response Structure:**
```typescript
interface SmartReplyResponse {
  drafts: {
    short: string;
    medium: string;
    detailed: string;
  };
}
```

### Integration with iOS

**iOS Integration will happen in Story 6.5:**
- Create `AIService.swift` with `generateSmartReplies()` method
- Add "Draft Reply" button to MessageThreadView
- Show SmartReplyPickerView with 3 options
- Populate message composer when user selects a draft

### Testing

**Testing Approach:**
1. Create creator profile manually in Firestore
2. Create test conversation with multiple messages in RTDB
3. Deploy `generateSmartReplies` function
4. Test via Firebase Console > Functions > generateSmartReplies > Test
5. Or test via iOS app once Story 6.5 is complete

**Test Cases:**
- Simple fan message: "I love your videos!" â†’ 3 thank-you responses with varying detail
- Technical question: "How do you set up Firebase?" â†’ 3 responses ranging from brief to detailed
- Business inquiry: "Interested in sponsorship collaboration" â†’ 3 professional responses
- Long conversation: Test with 20+ messages to verify context is used
- Empty conversation: Test with minimal context to verify function still works

**Expected Response Format:**
```json
{
  "drafts": {
    "short": "Thanks so much! ðŸ™Œ",
    "medium": "Hey! Really appreciate the kind words. Means a lot to know the content is helpful!",
    "detailed": "Hey! Thanks so much for watching and taking the time to reach out. It really means a lot to hear that the videos are helping you. Always trying to make the content as useful as possible, so feedback like this keeps me motivated! Let me know if there's anything specific you'd like to see covered. - Andrew"
  }
}
```

**Performance Testing:**
- Measure response time for various conversation lengths
- Target: <3 seconds for 20-message context
- Verify GPT-4o-mini is significantly faster than GPT-4 would be

**Testing Standards** [Source: architecture/testing-strategy.md - assumed]
- Manual testing via deployed function
- Test with real conversation context
- Verify authenticity of generated responses
- Verify response times meet targets
- No unit tests required for this story

### Cost Estimation

**GPT-4o-mini Pricing:**
- Input: $0.15 per 1M tokens
- Output: $0.60 per 1M tokens

**Per Smart Reply Request Cost:**
- Input: ~1500 tokens (creator profile + 20 messages + prompts)
- Output: ~150 tokens (3 reply drafts)
- Cost per request: ~$0.00032 (3.2 hundredths of a cent)

**Monthly Cost Estimate (50 smart reply requests/day):**
- 1,500 requests/month Ã— $0.00032 = $0.48/month
- Extremely affordable!

### Important Notes

- This story implements Feature 2 (Response Drafting) AND Advanced AI Capability (10 points!)
- Most valuable story in Epic 6 from a scoring perspective
- GPT-4o-mini provides excellent quality at 15x lower cost than GPT-4
- Response times <3s significantly exceed rubric requirement of <8s
- Context-aware design ensures replies are relevant, not generic
- User can edit drafts before sending (not fully automated)

### Prerequisites

- Story 6.0 (Environment Configuration) must be complete
- Story 6.1 (Firebase Cloud Functions Setup) must be complete
- OpenAI API key must be configured
- RTDB must have message data
- Firestore must be enabled

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-22 | 1.0 | Initial story creation | Scrum Master (Bob) |

## Dev Agent Record

*This section will be populated by the development agent during implementation.*

### Agent Model Used

*To be populated by dev agent*

### Debug Log References

*To be populated by dev agent*

### Completion Notes

*To be populated by dev agent*

### File List

*To be populated by dev agent*

## QA Results

*This section will be populated by the QA agent after story completion.*
