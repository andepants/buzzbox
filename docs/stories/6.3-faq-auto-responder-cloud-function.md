# Story 6.3: FAQ Auto-Responder Cloud Function (Feature 3)

## Status
Draft

## Story
**As a** fan,
**I want** instant answers to common questions,
**so that** I don't have to wait for Andrew to respond to frequently asked questions.

## Acceptance Criteria
1. 15 FAQs created in Firestore manually (no embeddings needed!)
2. `checkFAQ` function fetches all FAQs and sends to GPT-4o-mini
3. GPT-4o-mini matches questions semantically (handles different wording)
4. Returns exact FAQ answer when matched
5. iOS receives matchedQuestion in response
6. iOS auto-sends FAQ answers
7. AI-generated badge shows on auto-responses
8. FAQ check failures don't block message delivery
9. Response time <1s (GPT-4o-mini is FAST!)
10. No vector indexes needed
11. No embedding generation step needed

## Tasks / Subtasks

- [ ] Create comprehensive FAQ collection in Firestore (AC: 1)
  - [ ] Manually create all 15 FAQ documents in Firebase Console
  - [ ] Collection path: `/faqs/{faqId}`
  - [ ] Use meaningful document IDs: `faq-001`, `faq-002`, etc.
  - [ ] Each FAQ must have fields: `question`, `answer`, `category`, `order` (for sorting)
  - [ ] Categories: schedule, support, collaboration, technical, general
  - [ ] Complete FAQ list:
    1. faq-001: "What time do you stream?" → Schedule info
    2. faq-002: "How can I support you?" → Patreon/membership links
    3. faq-003: "Do you take collaboration requests?" → Collaboration policy
    4. faq-004: "What's your tech stack?" → Tech stack overview
    5. faq-005: "How do I submit content ideas?" → Suggestion process
    6. faq-006: "Where can I find your social media?" → Social links
    7. faq-007: "Do you offer consulting/coaching?" → Consulting info
    8. faq-008: "What camera/mic do you use?" → Equipment list
    9. faq-009: "How did you get started?" → Origin story
    10. faq-010: "Can I use your code in my project?" → Licensing info
    11. faq-011: "How often do you post?" → Content schedule
    12. faq-012: "Do you have a Discord server?" → Community links
    13. faq-013: "What courses do you recommend?" → Learning resources
    14. faq-014: "How do I report a bug?" → Bug reporting process
    15. faq-015: "Do you do sponsored content?" → Sponsorship policy
  - [ ] Populate with actual answers relevant to Andrew's platform

- [ ] Create Firestore security rules for FAQs
  - [ ] Create or update `firestore.rules` file
  - [ ] Add rules for `/faqs` collection:
    - Allow read for all authenticated users
    - Allow write only for admin/Cloud Functions service account
  - [ ] Deploy rules: `firebase deploy --only firestore:rules`
  - [ ] Test rules by attempting to read FAQ from iOS app

- [ ] Create FAQ Cloud Function file (AC: 2)
  - [ ] Create `functions/src/faq.ts`
  - [ ] Import: `firebase-functions/v2/https`, `firebase-admin`, `openai`
  - [ ] Initialize OpenAI client
  - [ ] Define FAQ and FAQResponse interfaces

- [ ] Implement checkFAQ callable function (AC: 2)
  - [ ] Create `checkFAQ` using `onCall` from firebase-functions/v2/https
  - [ ] Accept `text` parameter from request.data
  - [ ] Validate text parameter is provided and is string
  - [ ] Throw HttpsError if invalid input

- [ ] Fetch all FAQs from Firestore (AC: 2)
  - [ ] Query Firestore collection: `admin.firestore().collection('faqs').orderBy('order').get()`
  - [ ] Map snapshot to FAQ array
  - [ ] Check FAQ count and log warning if > 20
  - [ ] If count > 20, limit to first 20 by order field
  - [ ] Return `{ isFAQ: false }` if no FAQs exist in Firestore
  - [ ] Log FAQ count for monitoring

- [ ] Build FAQ context for GPT-4o-mini (AC: 2, 3)
  - [ ] Format FAQs as: `Q: {question}\nA: {answer}`
  - [ ] Join all FAQs with double newlines
  - [ ] Build system prompt with all FAQs as context
  - [ ] System prompt: "You are a helpful FAQ assistant. Given a user's question, determine if it matches any FAQ below. [FAQ context]. If question matches (even if worded differently), respond with: MATCHED_QUESTION: {original question}\nANSWER: {answer}. If no good match, respond with exactly: NO_MATCH"

- [ ] Call OpenAI for FAQ matching (AC: 3, 4)
  - [ ] Use GPT-4o-mini model
  - [ ] Temperature: 0.3 (deterministic)
  - [ ] Max tokens: 200 (enough for FAQ answers)
  - [ ] Parse response and check for 'NO_MATCH'

- [ ] Handle FAQ matching results (AC: 4, 5)
  - [ ] If response contains 'NO_MATCH', return `{ isFAQ: false }`
  - [ ] If matched, parse response for MATCHED_QUESTION and ANSWER
  - [ ] Extract matchedQuestion using regex or string parsing
  - [ ] Extract answer using regex or string parsing
  - [ ] If parsing fails, fallback: search FAQs for one containing response substring
  - [ ] Return `{ isFAQ: true, answer: extractedAnswer, matchedQuestion: extractedQuestion }`
  - [ ] Log matched question for analytics

- [ ] Add error handling (AC: 8)
  - [ ] Wrap in try/catch block
  - [ ] Log errors with `logger.error()`
  - [ ] Throw HttpsError('internal', 'FAQ check failed') on errors
  - [ ] Client should catch and degrade gracefully

- [ ] Export function and deploy (AC: 2)
  - [ ] Export `checkFAQ` in `functions/src/index.ts`
  - [ ] Deploy: `firebase deploy --only functions`
  - [ ] Verify deployment succeeds

- [ ] Test FAQ matching (AC: 3, 9)
  - [ ] Test exact match: "What time do you stream?" → should match
  - [ ] Test paraphrased: "When are your streams?" → should match
  - [ ] Test unrelated: "What's the weather?" → should return NO_MATCH
  - [ ] Verify response time is <1 second
  - [ ] Check function logs for performance

## Dev Notes

### Architecture Context

**AI Feature Flow** [Source: architecture/ai-integration-architecture.md#6.1]
- On-Demand (Callable) FAQ detection
- iOS app calls Cloud Function when user sends message
- Function returns FAQ answer if matched
- iOS app can auto-send the answer

**Cloud Functions Specifications** [Source: architecture/ai-integration-architecture.md#6.2]
- Function 3: `detectFAQCallable` (this story)
  - Match incoming message to FAQ library
  - Return suggested answer if confidence > 70%
  - Target execution: ~2 seconds (Epic 6 says <1s with GPT-4o-mini)

**Simplified Approach** [Source: docs/prd/epic-6-ai-powered-creator-inbox.md]
- With only 15 FAQs, send all FAQs to GPT-4 as context
- NO embeddings needed
- NO vector search needed
- NO vector indexes needed
- GPT-4o-mini handles semantic matching natively
- Much simpler implementation (30 min vs 1.5 hours)

### Firestore Security Rules

**Important:** Firestore security rules must allow Cloud Functions (Admin SDK) to read/write FAQs.

Security rules will be deployed in a separate story, but for this story:
- FAQs are read-only from client apps
- Only Cloud Functions (via Admin SDK) can write
- Manual creation via Firebase Console is allowed

### FAQ Data Structure

**Firestore Collection: `/faqs`**

Example FAQ document:
```json
{
  "question": "What time do you stream?",
  "answer": "I stream Monday-Friday at 7pm EST on YouTube! See you there 🎮",
  "category": "schedule"
}
```

**Sample 15 FAQs** [Source: docs/prd/epic-6-ai-powered-creator-inbox.md]:
1. "What time do you stream?" → "I stream Monday-Friday at 7pm EST on YouTube! See you there 🎮"
2. "How can I support you?" → "Thanks for asking! You can support through YouTube memberships, Patreon, or just sharing my content. Every bit helps! 🙏"
3. (Create 13 more similar FAQs in Firebase Console)

### File Locations

**New File to Create:**
- `functions/src/faq.ts` - FAQ Auto-Responder Cloud Function implementation

**Files to Modify:**
- `functions/src/index.ts` - Export `checkFAQ` function

**Firestore Collection:**
- `/faqs/{faqId}` - Created manually in Firebase Console

### Implementation Details

**Function Signature:**
```typescript
export const checkFAQ = onCall(async (request) => {
  const { text } = request.data;
  // Implementation here
});
```

**FAQ Context Building:**
```typescript
const faqContext = faqs
  .map((faq) => `Q: ${faq.question}\nA: ${faq.answer}`)
  .join('\n\n');
```

**OpenAI Call:**
```typescript
const completion = await openai.chat.completions.create({
  model: 'gpt-4o-mini',
  messages: [
    {
      role: 'system',
      content: `You are a helpful FAQ assistant. Given a user's question, determine if it matches any FAQ below.

Available FAQs:
${faqContext}

If the question matches an FAQ (even if worded differently), respond with ONLY the matched FAQ answer verbatim.
If no good match, respond with exactly: NO_MATCH`
    },
    {
      role: 'user',
      content: text
    }
  ],
  temperature: 0.3,
  max_tokens: 200,
});
```

**Response Structure:**
```typescript
interface FAQResponse {
  isFAQ: boolean;
  answer?: string;
  matchedQuestion?: string;
}
```

### Integration with iOS

**iOS Integration will happen in Story 6.5 and 6.6:**
- Story 6.5: Create `AIService.swift` with `checkFAQ()` method
- Story 6.6: Integrate FAQ auto-response in message receiving flow

**How it will work:**
1. User sends message to creator
2. iOS calls `checkFAQ` Cloud Function
3. If `isFAQ: true`, iOS auto-sends the answer
4. Auto-response is marked with `isAIGenerated: true`
5. UI shows "AI Response" badge (Story 6.7)

### Testing

**Testing Approach:**
1. Create 15 FAQs manually in Firebase Console
2. Deploy `checkFAQ` function
3. Test via Firebase Console > Functions > checkFAQ > Test
4. Or test via iOS app once Story 6.5 is complete

**Test Cases:**
- Exact match: "What time do you stream?" → should return FAQ answer
- Paraphrased: "When do you go live?" → should return FAQ answer
- Different wording: "Stream schedule?" → should return FAQ answer
- Unrelated question: "What's the weather today?" → should return `isFAQ: false`
- Empty string: "" → should throw validation error
- Null: null → should throw validation error

**Testing Standards** [Source: architecture/testing-strategy.md - assumed]
- Manual testing via deployed function
- Test exact matches and paraphrases
- Verify response times are <1 second
- No unit tests required for this story

### Cost Estimation

**GPT-4o-mini Pricing:**
- Input: $0.15 per 1M tokens
- Output: $0.60 per 1M tokens

**Per FAQ Check Cost:**
- Input: ~1500 tokens (15 FAQs + user question)
- Output: ~50 tokens (FAQ answer or NO_MATCH)
- Cost per check: ~$0.00026 (2.6 hundredths of a cent)

**Monthly Cost Estimate (100 FAQ checks/day):**
- 3,000 checks/month × $0.00026 = $0.78/month
- Still extremely cheap!

### Important Notes

- This story implements Feature 3 (FAQ Auto-Responder) of the 5 required AI features
- NO vector embeddings or indexes needed thanks to simplified approach
- GPT-4o-mini natively understands semantic similarity
- Response times are fast enough (<1s) without optimization
- Manual FAQ creation is acceptable for 15 FAQs
- Future enhancement: Add web UI for FAQ management

### Prerequisites

- Story 6.0 (Environment Configuration) must be complete
- Story 6.1 (Firebase Cloud Functions Setup) must be complete
- Firestore must be enabled in Firebase project
- OpenAI API key must be configured

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-01-22 | 1.0 | Initial story creation | Scrum Master (Bob) |

## Dev Agent Record

*This section will be populated by the development agent during implementation.*

### Agent Model Used

*To be populated by dev agent*

### Debug Log References

*To be populated by dev agent*

### Completion Notes

*To be populated by dev agent*

### File List

*To be populated by dev agent*

## QA Results

*This section will be populated by the QA agent after story completion.*
